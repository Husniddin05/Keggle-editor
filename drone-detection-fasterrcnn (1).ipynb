{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10017073,"sourceType":"datasetVersion","datasetId":6079696}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T12:16:09.932228Z","iopub.execute_input":"2024-12-13T12:16:09.932722Z","iopub.status.idle":"2024-12-13T12:16:10.503616Z","shell.execute_reply.started":"2024-12-13T12:16:09.932655Z","shell.execute_reply":"2024-12-13T12:16:10.502066Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# For CUDA 10.2\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu102","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T12:16:10.506219Z","iopub.execute_input":"2024-12-13T12:16:10.506980Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu102\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision.transforms import functional as FT\nfrom torchvision import transforms as T\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split, Dataset\nimport copy\nimport math\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import defaultdict, deque\nimport datetime\nimport time\nfrom tqdm import tqdm # progress bar\nfrom torchvision.utils import draw_bounding_boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.version.cuda)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pycocotools\nfrom pycocotools.coco import COCO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from albumentations.pytorch import ToTensorV2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transforms(train=False):\n    if train:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            A.HorizontalFlip(p=0.3),\n            A.VerticalFlip(p=0.3),\n            A.RandomBrightnessContrast(p=0.1),\n            A.ColorJitter(p=0.1),\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    else:\n        transform = A.Compose([\n            A.Resize(600, 600), # our input size can be 600px\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    return transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DroneDetection(datasets.VisionDataset):\n    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n        # the 3 transform parameters are reuqired for datasets.VisionDataset\n        super().__init__(root, transforms, transform, target_transform)\n        self.split = split #train, valid, test\n        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n    \n    def _load_image(self, id: int):\n        path = self.coco.loadImgs(id)[0]['file_name']\n        image = cv2.imread(os.path.join(self.root, self.split, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n    def _load_target(self, id):\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n    \n    def __getitem__(self, index):\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = self._load_target(id)\n        target = copy.deepcopy(self._load_target(id))\n        \n        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n        if self.transforms is not None:\n            transformed = self.transforms(image=image, bboxes=boxes)\n        \n        image = transformed['image']\n        boxes = transformed['bboxes']\n        \n        new_boxes = [] # convert from xywh to xyxy\n        for box in boxes:\n            xmin = box[0]\n            xmax = xmin + box[2]\n            ymin = box[1]\n            ymax = ymin + box[3]\n            new_boxes.append([xmin, ymin, xmax, ymax])\n        \n        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n        \n        targ = {} # here is our transformed target\n        targ['boxes'] = boxes\n        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n        return image.div(255), targ # scale images\n    def __len__(self):\n        return len(self.ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/drone-detection/coco json drone detection\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\ncategories = coco.cats\nn_classes = len(categories.keys())\ncategories","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = [i[1]['name'] for i in categories.items()]\nclasses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = DroneDetection(root=dataset_path, transforms=get_transforms(True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = train_dataset[2]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images,targets = next(iter(train_loader))\nimages = list(image for image in images)\ntargets = [{k:v for k, v in t.items()} for t in targets]\noutput = model(images, targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n\nmetrics={'epochs':[],'loss':[]}\ndef train_one_epoch(model, optimizer, loader, device, epoch):\n    model.to(device)\n    model.train()\n    all_losses = []\n    all_losses_dict = []\n    \n    for images, targets in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n        loss_value = losses.item()\n        \n        all_losses.append(loss_value)\n        all_losses_dict.append(loss_dict_append)\n        \n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping trainig\")\n            print(loss_dict)\n            sys.exit(1)\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n\n        \n    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n        all_losses_dict['loss_classifier'].mean(),\n        all_losses_dict['loss_box_reg'].mean(),\n        all_losses_dict['loss_rpn_box_reg'].mean(),\n        all_losses_dict['loss_objectness'].mean()\n    ))\n\n    metrics['loss'].append(np.mean(all_losses))\n    metrics['epochs'].append(epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs=30\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def iou_score(loader):\n    scores=[]\n    n=0\n    for images, targets in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        n+=len(images)\n        try:\n            x_target_min,y_target_min,x_target_max,y_target_max=targets[0]['boxes'].tolist()[0]\n        except:\n            continue\n        \n        prediction = model(images)\n        pred = prediction[0]\n\n        try:\n            x_min,y_min,x_max,y_max=pred['boxes'][pred['scores'] > 0.8].tolist()[0]\n        except:\n            continue\n\n        inter_x_min = max(x_min, x_target_min)\n        inter_y_min = max(y_min, y_target_min)\n        inter_x_max = min(x_max, x_target_max)\n        inter_y_max = min(y_max, y_target_max)\n\n        inter_width = max(0, inter_x_max - inter_x_min + 1)\n        inter_height = max(0, inter_y_max - inter_y_min + 1)\n        intersection_area = inter_width * inter_height\n\n        box1_area = (x_max - x_min + 1) * (y_max - y_min + 1)\n        box2_area = (x_target_max - x_target_min + 1) * (y_target_max - y_target_min + 1)\n\n        union_area = box1_area + box2_area - intersection_area\n\n        if union_area == 0:\n            score+=0\n\n        scores.append(intersection_area / union_area)\n\n    return sum(scores)/len(scores)\nprint(iou_score(train_loader))","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = DroneDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, _ = test_dataset[100]\nimg_int = torch.tensor(img*255, dtype=torch.uint8)\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n    pred = prediction[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 10))\nplt.imshow(draw_bounding_boxes(img_int,\n    pred['boxes'][pred['scores'] > 0.8],\n    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n).permute(1, 2, 0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(metrics['epochs'], metrics['loss'], marker='o', label='Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss vs. Epochs')\nplt.grid()\nplt.legend()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}